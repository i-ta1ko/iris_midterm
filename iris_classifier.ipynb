{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå∫ Beautiful Iris Dataset üå∫\n",
      "\n",
      "+----+---------------+--------------+---------------+--------------+-------------+\n",
      "| Id | SepalLengthCm | SepalWidthCm | PetalLengthCm | PetalWidthCm |   Species   |\n",
      "+----+---------------+--------------+---------------+--------------+-------------+\n",
      "| 1  |      5.1      |     3.5      |      1.4      |     0.2      | Iris-setosa |\n",
      "| 2  |      4.9      |     3.0      |      1.4      |     0.2      | Iris-setosa |\n",
      "| 3  |      4.7      |     3.2      |      1.3      |     0.2      | Iris-setosa |\n",
      "| 4  |      4.6      |     3.1      |      1.5      |     0.2      | Iris-setosa |\n",
      "| 5  |      5.0      |     3.6      |      1.4      |     0.2      | Iris-setosa |\n",
      "+----+---------------+--------------+---------------+--------------+-------------+\n",
      "\n",
      "üîç Checking for missing valuesüîç\n",
      "\n",
      "+---------------+----------------+\n",
      "|               | Missing Values |\n",
      "+---------------+----------------+\n",
      "|      Id       |       0        |\n",
      "| SepalLengthCm |       0        |\n",
      "| SepalWidthCm  |       0        |\n",
      "| PetalLengthCm |       0        |\n",
      "| PetalWidthCm  |       0        |\n",
      "|    Species    |       0        |\n",
      "+---------------+----------------+\n",
      "\n",
      " Random Forest Accuracy: 100.00% ................. ‚úÖ\n",
      "\n",
      " Classification Report: \n",
      "+--------------+-----------+--------+----------+---------+\n",
      "|              | precision | recall | f1-score | support |\n",
      "+--------------+-----------+--------+----------+---------+\n",
      "|      0       |    1.0    |  1.0   |   1.0    |  10.0   |\n",
      "|      1       |    1.0    |  1.0   |   1.0    |   9.0   |\n",
      "|      2       |    1.0    |  1.0   |   1.0    |  11.0   |\n",
      "|   accuracy   |    1.0    |  1.0   |   1.0    |   1.0   |\n",
      "|  macro avg   |    1.0    |  1.0   |   1.0    |  30.0   |\n",
      "| weighted avg |    1.0    |  1.0   |   1.0    |  30.0   |\n",
      "+--------------+-----------+--------+----------+---------+\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      " Logistic Regression Accuracy: 100.00% ........... ‚úÖ\n",
      "\n",
      " Classification Report: \n",
      "+--------------+-----------+--------+----------+---------+\n",
      "|              | precision | recall | f1-score | support |\n",
      "+--------------+-----------+--------+----------+---------+\n",
      "|      0       |    1.0    |  1.0   |   1.0    |  10.0   |\n",
      "|      1       |    1.0    |  1.0   |   1.0    |   9.0   |\n",
      "|      2       |    1.0    |  1.0   |   1.0    |  11.0   |\n",
      "|   accuracy   |    1.0    |  1.0   |   1.0    |   1.0   |\n",
      "|  macro avg   |    1.0    |  1.0   |   1.0    |  30.0   |\n",
      "| weighted avg |    1.0    |  1.0   |   1.0    |  30.0   |\n",
      "+--------------+-----------+--------+----------+---------+\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      " K-Nearest Neighbors Accuracy: 100.00% ........... ‚úÖ\n",
      "\n",
      " Classification Report: \n",
      "+--------------+-----------+--------+----------+---------+\n",
      "|              | precision | recall | f1-score | support |\n",
      "+--------------+-----------+--------+----------+---------+\n",
      "|      0       |    1.0    |  1.0   |   1.0    |  10.0   |\n",
      "|      1       |    1.0    |  1.0   |   1.0    |   9.0   |\n",
      "|      2       |    1.0    |  1.0   |   1.0    |  11.0   |\n",
      "|   accuracy   |    1.0    |  1.0   |   1.0    |   1.0   |\n",
      "|  macro avg   |    1.0    |  1.0   |   1.0    |  30.0   |\n",
      "| weighted avg |    1.0    |  1.0   |   1.0    |  30.0   |\n",
      "+--------------+-----------+--------+----------+---------+\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Predictions\n",
      "\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "#MIDTERM ASSIGNMENT\n",
    "\n",
    "#You will solve a Taxonomy problem. Use the Iris Species dataset (https://www.kaggle.com/datasets/uciml/iris/data) \n",
    "#The iris.csv files contains data of 3 iris flowers. The goal is to specify the specie of a flower given its dimensions (Sepal length, Sepal width, Petal length, Petal width). \n",
    "#Load the csv file and explore data, try to understand values, and look for missing values or any other problem (if any). \n",
    "#Prepare the data by separating into train and test data frames. The Y column is \"Species\" and it probably needs to be converted into numeric values, instead of text that it currently contains.\n",
    "#Select a classification algorithm, like random forest classifier, k-nearest neighbors, logistic regression, etc. You may want to try more than one.\n",
    "#Train the model and make predictions for your test data.\n",
    "#Evaluate your results.\n",
    "#Add comments in every step in your Jupyter Notebook and also add a conclusion at the end explaining your findings in a short paragraph.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#In this midterm I was trying to play arround with different visualization formats, i hope you don't mind, i was experimenting with tabulate, and it was quite a discovery\n",
    "#for me that i can use emojis!\n",
    "\n",
    "import pandas as pd \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tabulate import tabulate\n",
    "\n",
    "url = \"Iris.csv\"  \n",
    "df = pd.read_csv(url)\n",
    "\n",
    "#show data\n",
    "print(\"\\nüå∫ Beautiful Iris Dataset üå∫\\n\")\n",
    "print(tabulate(df.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "#checking data that missed\n",
    "print(\"\\nüîç Checking for missing valuesüîç\\n\")\n",
    "print(tabulate(pd.DataFrame(df.isnull().sum(), columns=['Missing Values']), headers='keys', tablefmt='pretty', showindex=True))\n",
    "\n",
    "#feature separation\n",
    "X = df.drop(\"Species\", axis=1)\n",
    "Y = df[\"Species\"]\n",
    "\n",
    "#convertt to number value\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "#scaling data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),  #add max_iter to fix error that i faced; \"User Increase the number of iterations (max_iter) or scale the data as shown in:...\"\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "#this thing check classifiers\n",
    "for name, model in classifiers.items():\n",
    "\n",
    "    #training models\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    " \n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "\n",
    "    #acuracy\n",
    "    print(f\"\\n{f' {name} Accuracy: {accuracy:.2%} ':.<50} ‚úÖ\")\n",
    "    \n",
    "    #class reports\n",
    "    print(\"\\n Classification Report: \")\n",
    "    report = classification_report(Y_test, predictions, output_dict=True)\n",
    "    print(tabulate(pd.DataFrame(report).transpose(), headers='keys', tablefmt='pretty'))\n",
    "\n",
    "    #separators\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "#and finally it's printing predictions\n",
    "print(\"Predictions\\n\")\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "#conclusion: based on the data we can see a high percentage occuracy, it was a good exercise to practise data extraction, also as i wrote at the beginning it's also cool how it can be made visually appealing \n",
    "#personally for me it's extremely important and data seems to be more clear and more readable\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
